\documentclass[11pt, a4paper, abstraction]{scrartcl}
%Weitere Optionen: twocolumn, twoside
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% Fonts and formating
\usepackage{amsmath}
\usepackage{amsfonts}

\newtheorem{example}{Example}
\newcommand{\term}[1]{\textit{#1}}

\title{Exploratory Insights into the Use of Shallow Semantics for Document Retrieval with Queries in Context}
\author{Angeliki Lazaridou, Mikhail Kozhenikov, Iliana Simova, Tassilo Barth \\\{angeliki, mikhail, ilianas, tbarth\}@coli.uni-sb.de}
\date{September 30, 2011}
\subject{Student Project Report}
\subtitle{Basic Algorithms in Computational Linguistics \\ %
		Taught by Prof. Martin Kay}

\begin{document}

\maketitle

\begin{abstract}
We explore the effect of two shallow semantic document representations on the precision of document retrieval, namely lexical chains and topic models. We focus on the special case where queries consisting of a few search terms are embedded into a larger context, whose size ranges from a few sentences to whole documents. The context helps to create and disambiguate a sequential semantic fingerprint of the query. This representation is matched against an index built over the document collection. We use suffix trees as an index data structure and an approximate matching algorithm to allow for minor mutations of the query sequence. To evaluate the proposed method, precision and recall on the TREC-9 interactive track data set are compared to a baseline. In short, our project outcome encompasses implementations of algorithms for finding lexical chains, suffix tree creation and approxmate matching on suffix trees. Additionally, we built interfaces to existing programs as well as a pipeline to coordinate the different components.
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction and Motivation}

Extended abstract plus possible applications -- discuss (very) shortly: why topic models, why lexical chains, why suffix trees, i.e. how might they help. \\
Also overview.

\section{Methodology}

Refer to related work here. Describe in detail how each part affects the original documents and its role in the overall process. Also motivate decisions wrt models and algorithms (e.g. why do we use LDA (?), why do we use the lexical chain algorithm we use etc.)

\subsection{Document Retrieval and Queries in Context}

\subsection{Lexical Chains}

Lexical chains are sequences of content words in a text that are semantically similar to each other. Semantic similarity is determined by lexical-semantic relations such as synonymy, holonymy and hyponymy. Consider the following example: ``The new convertible proves to be a good car. The motor is the strongest in the history of automobile.''
A lexical chain based on the relations mentioned would comprise ``convertible'', ``car'', ``motor'', and ``automobile''. Lexical chains can be seen as a representation for the lexical cohesion within a text. Our intutition is that, given a query together with a larger context, relevant documents are lexically more cohesive with the query. In a \\
Our approach (no large part of related work, I think)
Most approaches to lexical chaining so far used only nouns (which is probably due to the comparatively rich lexical resources capturing semantic relations between nouns), though other parts of speech could be included as well. \\ 



\subsection{Topic Models}
Topic modeling provides methods for automatically organizing, understanding, searching, and summarizing large set of collections of documents. Furthermore, they have the ability to discover the hidden themes (topics) that exists in such a collection and express them in terms of statistics. In this probabilistic framework, data are assumed to be observed from a generative probabilistic process that includes hidden variables and more precisely the hidden variables correspond to the thematic structure. In order to identify the topics that describe this collection, we need to infer the hidden structure using posterior inference, and finaly, for every new document we can estimate what is the current assigment of topics that we have learned from teh collection. 
For the needs of this project, we are using Latent Dirichlet Allocation (LDA), which is a generative algorithm which models every topic as a distribution od words and finally, every document as a mixture of topics. Then, every word of every documnt is drawn from one of these topics. 
The intutition for incorporating topics in our retrieval model, is that we expect that the topics will guide the query to ``fit'' in a topic. Moreover, we expect that relevant documents will share common topics with the query.

\subsection{Suffix Trees and Approximate Matching}

\section{Implementation}

Describes the pipeline, the tools used, the CONLL format

\subsection{Preprocessing}

\subsubsection{Tokenization, POS tagging, Lemmatization..}
\subsubsection{Term Extraction}

\subsection{Creation of the Topic Model}

For creating the topic models, we used the java-based MALLET toolkit. For using mallet with our documents that are in the CoNLL, we had to apply a number of preprocessing steps for transorming the CoNLL format into feature vectors, and then after the estimation of the model, we had to write again the learnt topics back in the CoNLL format. Finally, there is a special preprocessing if we want to force our model assigning the same topics in multi-word terms (e.g iron deficiency).

The functionallity of the topic modeling is dual. First of all, given a set of documents we estimate the model. That is we  learn the parameters $\theta$ and $\phi$. Then, for every query with a context, we run the inference step, and given the learnt model, we assign the topics in the context of the query. 

\subsection{Lexical Chain Building}

- Generic graph model of lexical cohesion \\
- Concrete implementations: Galley/McKeown (focused on WSD), Silber/McCoy \\
- Can handle: Chunks, additional terms, ..? 

\subsection{Index Creation and Search Process}

\section{Usage}

How to run it

\section{Evaluation}

\subsection{Setup}

Data set, evaluation measures, which steps carried out on which part of the data set

\subsection{Results}

Lots of graphs and tables plus some describing text

\subsection{Analysis}

Why do the graphs look the way they look

\section{Conclusion}

Summarize ToC, contributions, evaluation, outlook

\end{document}
