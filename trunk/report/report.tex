\documentclass[11pt, a4paper, abstraction]{scrartcl}
%Weitere Optionen: twocolumn, twoside
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% Fonts and formating
\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{algorithmic}
\usepackage{algorithm}

\newtheorem{example}{Example}
\newcommand{\term}[1]{\textit{#1}}

\title{Exploratory Insights into the Use of Shallow Semantics for Document Retrieval with Queries in Context}
\author{Angeliki Lazaridou, Mikhail Kozhenikov, Iliana Simova, Tassilo Barth \\\{angeliki, mikhail, ilianas, tbarth\}@coli.uni-sb.de}
\date{September 30, 2011}
\subject{Student Project Report}
\subtitle{Basic Algorithms in Computational Linguistics \\ %
		Taught by Prof. Martin Kay}

\begin{document}

\maketitle

\begin{abstract}
We explore the effect of two shallow semantic document representations on the precision of document retrieval, namely lexical chains and topic models. We focus on the special case where queries consisting of a few search terms are embedded into a larger context, whose size ranges from a few sentences to whole documents. The context helps to create and disambiguate a sequential semantic fingerprint of the query. This representation is matched against an index built over the document collection. We use suffix trees as an index data structure and an approximate matching algorithm to allow for minor mutations of the query sequence. To evaluate the proposed method, precision and recall on the TREC-9 interactive track data set are compared to a baseline. In short, our project outcome encompasses implementations of algorithms for finding lexical chains, suffix tree creation and approxmate matching on suffix trees. Additionally, we built interfaces to existing programs as well as a pipeline to coordinate the different components.
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction and Motivation}

Extended abstract plus possible applications -- discuss (very) shortly: why topic models, why lexical chains, why suffix trees, i.e. how might they help. \\
Also overview.

\section{Methodology}

Refer to related work here. Describe in detail how each part affects the original documents and its role in the overall process. Also motivate decisions wrt models and algorithms (e.g. why do we use LDA (?), why do we use the lexical chain algorithm we use etc.)

\subsection{Document Retrieval and Queries in Context}

\subsection{Lexical Chains}

Lexical chains are sequences of content words in a text that are semantically similar to each other. Semantic similarity is determined by lexical-semantic relations such as synonymy, holonymy and hyponymy. Consider the following example: ``The new convertible proves to be a good car. The motor is the strongest in the history of automobile.''
A lexical chain based on the relations mentioned would comprise ``convertible'', ``car'', ``motor'', and ``automobile''. Lexical chains can be seen as a representation for the lexical cohesion within a text. Our intutition is that, given a query together with a larger context, relevant documents are lexically more cohesive with the query. In a \\
Our approach (no large part of related work, I think)
Most approaches to lexical chaining so far used only nouns (which is probably due to the comparatively rich lexical resources capturing semantic relations between nouns), though other parts of speech could be included as well. \\ 



\subsection{Topic Models}
Topic modeling provides methods for automatically organizing, understanding, searching, and summarizing large set of collections of documents. Furthermore, they have the ability to discover the hidden themes (topics) that exists in such a collection and express them in terms of statistics. In this probabilistic framework, data are assumed to be observed from a generative probabilistic process that includes hidden variables and more precisely the hidden variables correspond to the thematic structure. In order to identify the topics that describe this collection, we need to infer the hidden structure using posterior inference, and finaly, for every new document we can estimate what is the current assigment of topics that we have learned from teh collection. 
For the needs of this project, we are using Latent Dirichlet Allocation (LDA), which is a generative algorithm which models every topic as a distribution od words and finally, every document as a mixture of topics. Then, every word of every documnt is drawn from one of these topics. 
The intutition for incorporating topics in our retrieval model, is that we expect that the topics will guide the query to ``fit'' in a topic. Moreover, we expect that relevant documents will share common topics with the query.

\subsection{Suffix Trees and Clustering}
Though our original intention was to retrieve the documents using suffix trees, it turned out to be impractical with respect to the dataset in question, since the queries are long and not necessarily well-formed, which renders exact search useless. Approximate search could serve as a partial solution, but we feel that the results would still offer a recall too low for the task in question.

We therefore make use of suffix trees in a different fashion -- to retrieve more, rather than less documents -- by implementing a clustering algorithm based on suffix trees and retrieving whole clusters, rather than single documents. This allows us to retrieve documents similar to those containing the query terms and can therefore be seen as a method of query expansion. The clustering algorithm is that by Zamir and Etzioni, as described in ``Web Document Clustering: A Feasibility Demonstration''. Its main advantage over most other clustering algorithms is the ability to consider shared phrases rather than single words, while maintaining linear time complexity, though the latter is achieved by using a beam search-like approximation.

\section{Implementation}

\subsection{Preprocessing}

\subsubsection{Tokenization, POS tagging, Lemmatization}
These steps were carried out by the TreeTagger \footnote{http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/}. The result is written in a tab-separated format (CONLL format) into files on the disk.

\subsubsection{Term Extraction}

???

\subsection{Creation of the Topic Model}

For creating the topic models, we used the java-based MALLET toolkit. For using mallet with our documents that are in the CoNLL, we had to apply a number of preprocessing steps for transorming the CoNLL format into feature vectors, and then after the estimation of the model, we had to write again the learnt topics back in the CoNLL format. Finally, there is a special preprocessing if we want to force our model assigning the same topics in multi-word terms (e.g iron deficiency).

The functionality of the topic modeling is dual. First of all, given a set of documents we estimate the model. That is we  learn the parameters $\theta$ and $\phi$. Then, for every query with a context, we run the inference step, and given the learnt model, we assign the topics in the context of the query. 

\subsection{Lexical Chain Building}

We implemented two lexical chaining algorithms, Silber/McCoy (2003) and Galley/McKeown (2003). Both of them are very similar and have linear time and space requirements with regard to document size, but Galley/McKeown imposes stricter word sense disambiguation. \\
The basic components as well as the first processing step of the algorithms are identical. Word occurrences in a text or their assumed word senses respectively are treated as lexical nodes in a undirected graph. Together with word sense number and the word token, we store the exact position of the node in the text. A different kind of nodes in the graph are so-called meta chains: They are created implicitly for each WordNet sense encountered while processing a text, as well as for all terms not found in WordNet. This means that each lexical node also ``owns'' a meta chain with the corresponding sense. Meta chains retain the sequential order of heir lexical nodes, i.e. the order in which the nodes entered the chain. \\
The first processing step inserts edges between the lexical nodes created for each word and the meta chains representing senses in the transitive WordNet closure of said nodes. The closure comprises synonyms, hyponyms, hyperonyms and siblings in the sense hierarchy. All edges are tagged with their relation type (e.g. synonymy). At the end of the first step, we are left with lexical nodes representing all possible senses of the nouns in the text, and meta chain nodes associated to certain senses holding ordered sequences of semantically related lexical nodes. Each lexical node is implicitly linked to all other nodes it shares a chain with. Their relation type is either synonymy/identity (if their word or sense number is the same) or given by the type of the edge between of one of the nodes and the chain of the other. If a such edge does not exist, the nodes are not semantically related. These edges could also be created explicitly. However, in our implementation we refrain from this for efficiency reasons. \\
At the second step, the two algorithms diverge. Galley/McKeown impose a strong ``One sense per discourse'' assumption: It assigns a score to all lexical nodes of a distinct word (not word instance!) and selects the highest-scoring one. This is assumed to be the most likely sense of the word for the whole text. Thus, the algorithm removes all other senses (lexical nodes) of the word from the lexical graph. The score of a lexical node is the summed score of all of its implicit links to other lexical nodes. An edge is scored according to a very simple step function of the relation type and the distance in the text between the two nodes. For details of the scoring function, please refer to the original paper. Apart from the ones representing the dominant senses, all other lexical nodes are removed from the meta chains. The reduced chains are the final output. \\
On the contrary, Silber/McCoy allow for one word to have different senses and generally follow a more ``chain-centric'' approach: Their algorithm determines for each word instance the chain on which its membership has the highest influence, using a similar scoring function as the other algorithm. All lexical nodes of the word instance not belonging to the selected chain are removed. \\
Below we specify the algorithms in pseudo code. The first part is shared by both, while the second differs. EXPAND-BFS is a simple breadth-first search on the WordNet synset tree, starting from the given synset. SCORE is a step function of semantical relation type and text distance as specified in the corresponding papers. \\

Our implementation is written in Python and based on the Natural Language Toolkit (NLTK, http://www.nltk.org). We especially use its WordNet interface. Also, our algorithms expect a tokenized and POS-tagged text as input, where nouns must be tagged by symbols starting with a ``N''.  Details are given in the Usage section. The implementation emphasizes the generic nature of different lexical chaining algorithms, providing a hopefully useful interface to implement further variations. We plan to release the code under a GPL license by submitting it as a contribution to NLTK.

\begin{algorithm}
\caption{Lexical linking}
\begin{algorithmic}
	\STATE $MetaChains \gets$ Dictionary(Identifier to Lists)
	\FORALL {chunks $w$ in text}
		\STATE $position$ $\gets$ current word, sentence, paragraph index in text
		\IF {lemma $w$ $\in$ WordNet}
			\FORALL {synsets $s$, $w \in s$}
				\STATE $ln$ $\gets$ LexicalNode($s$, $w$, $position$)
				\FORALL {sense $s'$, relation $r$ in EXPAND-BFS(s)}
					\STATE Create edge between $ln$ and $MetaChains[s']$, tag it with $r$
				\ENDFOR
			\ENDFOR
		\ELSE
			\STATE Create edge between $ln$ and $MetaChains[w]$
		\ENDIF
	\ENDFOR
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Chain creation according to Silber/McCoy 2003}
\begin{algorithmic}
	\FORALL {chunk w in text}
		\FORALL {lexical nodes $ln$ associated with $w$}
			\FORALL {chains $c$, $ln \in c$}
				\STATE find predecessor node $lnp$ of $ln$ in $c$
				\STATE $s \gets $ SCORE($ln$, $lnp$)
				\STATE set $cm$ to $c$ for which $s$ is maximal
			\ENDFOR
		\ENDFOR
		\STATE remove $ln$ from all chains $\neq cm$
	\ENDFOR
	\RETURN {MetaChains}
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\begin{algorithmic}
\caption{Chain creation according to Galley/McKeown 2003}
	\FORALL {distinct chunks $w$}
		\FORALL {lexical nodes $ln$ associated with $w$}
			\STATE $score \gets 0$
			\FORALL {chains $c$, $ln \in c$} 
				\FORALL {$lno \in c, lno \neq ln$}
					\STATE $score \gets score$ + SCORE($ln$, $lno$)
				\ENDFOR
			\ENDFOR
			\STATE set $lnm$ to $ln$ for which $score$ is maximal 
		\ENDFOR
		\STATE remove all nodes representing different senses of $w$ than $lnm$ from all chains
	\ENDFOR 
	\RETURN {MetaChains}
\end{algorithmic}
\end{algorithm}

\subsection{Suffix Tree Clustering}

\subsubsection{Suffix Tree Construction}
We have implemented the suffix tree construction algorithm in Java following Ukkonen, 1995. It appears that most existing implementations are designed to work with individual symbols, rather than words, and are not always easily extensible. We were unable to locate an open source Java implementation of suffix trees suitable for our purposes. 

Java has been chosen as a trade-off between efficiency and portability. Our implementation doesn't use a number of known ways to minimize the memory requirements, since it is intended more for educational purposes than large-scale search. It does, however, maintain a linear-time worst case complexity\footnote{assuming the addition and retrieval of elements in the Java implementation of HashMap take O(1) time} and is capable of building the index for the dataset in question (over 50 thousand documents) in less than a minute.

We also use a well-known modification of the suffix tree construction algorithm to handle multiple documents -- each document ends with a unique end marker and the tree growth is stopped at each such marker.

\subsubsection{Clustering}
We follow the Suffix Tree Clustering algorithm of Zamir and Etzioni almost exactly in implementing the clustering algorithm. One aspect that is not described in the paper is the way the base cluster scores are combined to form merged cluster scores, so for the moment we obtain the latter by summing up the former.
 
\subsection{Index Creation and Search Process}

\section{Usage}

How to run it

% \section{Evaluation}
% 
% \subsection{Setup}
% 
% Data set, evaluation measures, which steps carried out on which part of the data set
% 
% \subsection{Results}
% 
% Lots of graphs and tables plus some describing text
% 
% \subsection{Analysis}
% 
% Why do the graphs look the way they look

\section{Conclusion}

Summarize ToC, contributions, evaluation, outlook

\end{document}
